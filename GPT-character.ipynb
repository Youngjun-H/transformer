{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MaskedSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, atten_dim):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(embed_dim, atten_dim, bias=False)\n",
    "        self.key = nn.Linear(embed_dim, atten_dim, bias=False)\n",
    "        self.value = nn.Linear(embed_dim, atten_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "        \n",
    "        scores = torch.matmul(query, key.transpose(-2, -1))\n",
    "        scores = scores / key.size(-1)**0.5\n",
    "\n",
    "        tril = torch.tril(torch.ones(x.size(1), x.size(1))).to(x.device)\n",
    "        masked_scores = scores.masked_fill(tril==0, float('-inf'))\n",
    "\n",
    "        attention_weights = F.softmax(masked_scores, dim=-1)\n",
    "        weighted_values = torch.matmul(attention_weights, value)\n",
    "\n",
    "        return weighted_values\n",
    "\n",
    "class MultiHeadMaskedSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()        \n",
    "        atten_dim = embed_dim // num_heads\n",
    "        self.heads = nn.ModuleList([MaskedSelfAttention(embed_dim, atten_dim) for _ in range(num_heads)])\n",
    "        self.fc = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        head_outputs = []\n",
    "        for head in self.heads:\n",
    "            head_outputs.append(head(x))\n",
    "        concatenated_heads = torch.cat(head_outputs, dim=-1)\n",
    "        return self.fc(concatenated_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat May 24 17:57:42 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        Off | 00000000:01:00.0 Off |                  N/A |\n",
      "| 30%   28C    P8              13W / 350W |     26MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A       998      G   /usr/lib/xorg/Xorg                            9MiB |\n",
      "|    0   N/A  N/A      1205      G   /usr/bin/gnome-shell                          6MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, atten_dim):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(embed_dim, atten_dim)\n",
    "        self.query = nn.Linear(embed_dim, atten_dim)\n",
    "        self.value = nn.Linear(embed_dim, atten_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        key = self.key(x)\n",
    "        query = self.query(x)\n",
    "        value = self.value(x)\n",
    "\n",
    "        scores = torch.matmul(query, key.transpose(-2,-1))\n",
    "        scores = scores / key.size(-1) ** 0.5\n",
    "        atten_weight = F.softmax(scores, -1)\n",
    "\n",
    "        weighted_value = torch.matmul(atten_weight, value)\n",
    "\n",
    "        return weighted_value\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        atten_dim = embed_dim // num_heads\n",
    "        self.heads = nn.ModuleList([SelfAttention(embed_dim, atten_dim) for _ in range(num_heads)])\n",
    "        self.fc = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        head_outputs = []\n",
    "        for head in self.heads:\n",
    "            head_outputs.append(head(x))\n",
    "        concated_heads = torch.cat(head_outputs, dim=-1)\n",
    "        return self.fc(concated_heads)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, ff_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.mha = MultiHeadAttention(embed_dim, num_heads)\n",
    "        \n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.ff = FeedForward(embed_dim, ff_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.mha(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MaskedSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, atten_dim):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(embed_dim, atten_dim, bias=False)\n",
    "        self.key = nn.Linear(embed_dim, atten_dim, bias=False)\n",
    "        self.value = nn.Linear(embed_dim, atten_dim, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "        \n",
    "        scores = torch.matmul(query, key.transpose(-2,-1))\n",
    "        scores = scores / key.size(-1) ** 0.5\n",
    "\n",
    "        trill = torch.trill(torch.ones(x.size(1), x.size(1))).to(x.device)\n",
    "        masked_scores = scores.masked_fill(trill == 0, float('-inf'))\n",
    "        atten_weight = F.softmax(masked_scores, -1)\n",
    "\n",
    "        weighted_values = torch.matmul(atten_weight, value)\n",
    "        return weighted_values\n",
    "\n",
    "class MultiHeadMaskedSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        atten_dim = embed_dim // num_heads\n",
    "        self.heads = nn.ModuleList([MaskedSelfAttention(embed_dim, atten_dim) for _ in range(num_heads)])\n",
    "        self.fc = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        head_outputs = []\n",
    "        for head in self.heads:\n",
    "            head_outputs.append(head(x))\n",
    "        concatenated_heads = torch.cat(head_outputs, dim=-1)\n",
    "        return self.fc(concatenated_heads)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, ff_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class TransformerDecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.mha = MultiHeadMaskedSelfAttention(embed_dim, num_heads)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.ff = FeedForward(embed_dim, embed_dim*4)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.mha(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerGen(nn.Module):\n",
    "    def __init__(self, char_size, embed_dim, n_heads, n_layers, block_size):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.char_embedding = nn.Embedding(char_size, embed_dim)\n",
    "        self.pos_embedding = nn.Embedding(block_size, embed_dim)\n",
    "        self.transformer_blocks = nn.ModuleList(*[TransformerDecoderBlock(embed_dim, n_heads) for _ in range(n_layers)])\n",
    "        self.ln_f = nn.LayerNorm(embed_dim)\n",
    "        self.fc = nn.Linear(embed_dim, char_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        char_embeddings = self.char_embedding(x)\n",
    "        positions = torch.arange(0, x.size(1)).to(x.device).unsqueeze(0)\n",
    "        pos_embeddings = self.pos_embedding(positions)\n",
    "        x = char_embeddings + pos_embeddings\n",
    "        x = self.transformer_blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "    \n",
    "    def generate():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embed = 32\n",
    "n_heads = 4\n",
    "n_layers = 4\n",
    "block_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KJV\n",
      "King James Bible: Pure Cambridge Edition - Text courtesy of www.BibleProtector.com\n",
      "Genesis 1:1\tIn the beginning God created the heaven and the earth.\n",
      "Genesis 1:2\tAnd the earth was without form, and void; and darkness [was] upon the face of the deep. And the Spirit of God moved upon the face of the waters.\n",
      "Genesis 1:3\tAnd God said, Let there be light: and there was light.\n",
      "Genesis 1:4\tAnd God saw the light, that [it was] good: and God divided the light from the darkness.\n",
      "Genesis 1:5\tAnd God called the light Day, and the darkness he called Night. And the evening and the morning were the first day.\n",
      "Genesis 1:6\tAnd God said, Let there be a firmament in the midst of the waters, and let it divide the waters from the waters.\n",
      "Genesis 1:7\tAnd God made the firmament, and divided the waters which [were] under the firmament from the waters which [were] above the firmament: and it was so.\n",
      "Genesis 1:8\tAnd God called the firmament Heaven. And the evening and the morning were the second day.\n",
      "Genesi\n"
     ]
    }
   ],
   "source": [
    "with open(\"deepLearning/transformer/bible.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\n",
      " !(),-.0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWYZ[]abcdefghijklmnopqrstuvwxyz—’\n",
      "77\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"\".join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda x: \"\".join(itos[i] for i in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = encode(text)\n",
    "\n",
    "data = torch.tensor(encoded_text, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4602957"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "def get_batch(data, batch_size, block_size):\n",
    "    ix = torch.randint(0, data.shape[0] - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
